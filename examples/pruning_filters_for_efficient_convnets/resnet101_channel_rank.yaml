# This is an example of one-shot channel pruning.
# It is very similar to the pruning schedule described in
#   Pruning Filters for Efficient Convnets, H. Li, A. Kadav, I. Durdanovic, H. Samet, and H. P. Graf.
#   ICLR 2017, arXiv:1608.087
# However, instead of one-shot filter ranking and pruning, we perform one-shot channel ranking and
# pruning, using L1-magnitude of the structures.
#
# time python3 compress_classifier.py -a=resnet56_cifar 
# -p=50 ../../../data.cifar10 --epochs=70 --lr=0.1 
# --compress=../pruning_filters_for_efficient_convnets/resnet56_cifar_channel_rank.yaml 
# --resume-from=checkpoint.resnet56_cifar_baseline.pth.tar --reset-optimizer --vs=0

# time python3 compress_classifier.py -a=vgg19 -p=50 
# ../../../data.imagenet --epochs=10 --lr=0.00001 
# --compress=../pruning_filters_for_efficient_convnets/vgg19.schedule_filter_rank.yaml 
# --pretrained

# time python examples/classifier_compression/compress_classifier.py \
# -a=resnet101 -p=50 \
# /home/hongky/datasets/imagenet \
# --epochs=10 --lr=0.00001 \
# --compress=examples/pruning_filters_for_efficient_convnets/resnet101_channel_rank.yaml \
# --pretrained

#
# Baseline results:
#     Top1: 92.850    Top5: 99.780    Loss: 0.464
#     Parameters: 851,504
#     Total MACs: 125,747,840
#
# Results:
#     Top1: 92.580    Top5: 99.670    Loss: 0.378
#     Parameters: 566,887 (=33.4% sparse)
#     Total MACs: 66,592,384  (=1.89x less MACs)
#
# Parameters:
# 	Name	Type	Attrs	IFM	IFM volume	OFM	OFM volume	Weights volume	MACs
# 0	conv1	Conv2d	k=(7, 7)	(1, 3, 224, 224)	150528	(1, 64, 112, 112)	802816	9408	118013952
# 1	layer1.0.conv1	Conv2d	k=(1, 1)	(1, 64, 56, 56)	200704	(1, 64, 56, 56)	200704	4096	12845056
# 2	layer1.0.conv2	Conv2d	k=(3, 3)	(1, 64, 56, 56)	200704	(1, 64, 56, 56)	200704	36864	115605504
# 3	layer1.0.conv3	Conv2d	k=(1, 1)	(1, 64, 56, 56)	200704	(1, 256, 56, 56)	802816	16384	51380224
# 4	layer1.0.downsample.0	Conv2d	k=(1, 1)	(1, 64, 56, 56)	200704	(1, 256, 56, 56)	802816	16384	51380224
# 5	layer1.1.conv1	Conv2d	k=(1, 1)	(1, 256, 56, 56)	802816	(1, 64, 56, 56)	200704	16384	51380224
# 6	layer1.1.conv2	Conv2d	k=(3, 3)	(1, 64, 56, 56)	200704	(1, 64, 56, 56)	200704	36864	115605504
# 7	layer1.1.conv3	Conv2d	k=(1, 1)	(1, 64, 56, 56)	200704	(1, 256, 56, 56)	802816	16384	51380224
# 8	layer1.2.conv1	Conv2d	k=(1, 1)	(1, 256, 56, 56)	802816	(1, 64, 56, 56)	200704	16384	51380224
# 9	layer1.2.conv2	Conv2d	k=(3, 3)	(1, 64, 56, 56)	200704	(1, 64, 56, 56)	200704	36864	115605504
# 10	layer1.2.conv3	Conv2d	k=(1, 1)	(1, 64, 56, 56)	200704	(1, 256, 56, 56)	802816	16384	51380224


# 11	layer2.0.conv1	Conv2d	k=(1, 1)	(1, 256, 56, 56)	802816	(1, 128, 56, 56)	401408	32768	102760448
# 12	layer2.0.conv2	Conv2d	k=(3, 3)	(1, 128, 56, 56)	401408	(1, 128, 28, 28)	100352	147456	115605504
# 13	layer2.0.conv3	Conv2d	k=(1, 1)	(1, 128, 28, 28)	100352	(1, 512, 28, 28)	401408	65536	51380224
# 14	layer2.0.downsample.0	Conv2d	k=(1, 1)	(1, 256, 56, 56)	802816	(1, 512, 28, 28)	401408	131072	102760448
# 15	layer2.1.conv1	Conv2d	k=(1, 1)	(1, 512, 28, 28)	401408	(1, 128, 28, 28)	100352	65536	51380224
# 16	layer2.1.conv2	Conv2d	k=(3, 3)	(1, 128, 28, 28)	100352	(1, 128, 28, 28)	100352	147456	115605504
# 17	layer2.1.conv3	Conv2d	k=(1, 1)	(1, 128, 28, 28)	100352	(1, 512, 28, 28)	401408	65536	51380224
# 18	layer2.2.conv1	Conv2d	k=(1, 1)	(1, 512, 28, 28)	401408	(1, 128, 28, 28)	100352	65536	51380224
# 19	layer2.2.conv2	Conv2d	k=(3, 3)	(1, 128, 28, 28)	100352	(1, 128, 28, 28)	100352	147456	115605504
# 20	layer2.2.conv3	Conv2d	k=(1, 1)	(1, 128, 28, 28)	100352	(1, 512, 28, 28)	401408	65536	51380224
# 21	layer2.3.conv1	Conv2d	k=(1, 1)	(1, 512, 28, 28)	401408	(1, 128, 28, 28)	100352	65536	51380224
# 22	layer2.3.conv2	Conv2d	k=(3, 3)	(1, 128, 28, 28)	100352	(1, 128, 28, 28)	100352	147456	115605504
# 23	layer2.3.conv3	Conv2d	k=(1, 1)	(1, 128, 28, 28)	100352	(1, 512, 28, 28)	401408	65536	51380224


# 24	layer3.0.conv1	Conv2d	k=(1, 1)	(1, 512, 28, 28)	401408	(1, 256, 28, 28)	200704	131072	102760448
# 25	layer3.0.conv2	Conv2d	k=(3, 3)	(1, 256, 28, 28)	200704	(1, 256, 14, 14)	50176	589824	115605504
# 26	layer3.0.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 27	layer3.0.downsample.0	Conv2d	k=(1, 1)	(1, 512, 28, 28)	401408	(1, 1024, 14, 14)	200704	524288	102760448
# 28	layer3.1.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 29	layer3.1.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 30	layer3.1.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 31	layer3.2.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 32	layer3.2.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 33	layer3.2.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 34	layer3.3.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 35	layer3.3.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 36	layer3.3.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 37	layer3.4.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 38	layer3.4.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 39	layer3.4.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 40	layer3.5.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 41	layer3.5.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 42	layer3.5.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 43	layer3.6.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 44	layer3.6.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 45	layer3.6.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 46	layer3.7.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 47	layer3.7.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 48	layer3.7.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 49	layer3.8.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 50	layer3.8.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 51	layer3.8.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 52	layer3.9.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 53	layer3.9.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 54	layer3.9.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 55	layer3.10.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 56	layer3.10.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 57	layer3.10.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 58	layer3.11.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 59	layer3.11.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 60	layer3.11.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 61	layer3.12.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 62	layer3.12.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 63	layer3.12.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 64	layer3.13.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 65	layer3.13.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 66	layer3.13.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 67	layer3.14.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 68	layer3.14.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 69	layer3.14.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 70	layer3.15.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 71	layer3.15.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 72	layer3.15.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 73	layer3.16.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 74	layer3.16.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 75	layer3.16.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 76	layer3.17.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 77	layer3.17.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 78	layer3.17.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 79	layer3.18.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 80	layer3.18.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 81	layer3.18.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 82	layer3.19.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 83	layer3.19.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 84	layer3.19.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 85	layer3.20.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 86	layer3.20.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 87	layer3.20.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 88	layer3.21.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 89	layer3.21.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 90	layer3.21.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224
# 91	layer3.22.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 256, 14, 14)	50176	262144	51380224
# 92	layer3.22.conv2	Conv2d	k=(3, 3)	(1, 256, 14, 14)	50176	(1, 256, 14, 14)	50176	589824	115605504
# 93	layer3.22.conv3	Conv2d	k=(1, 1)	(1, 256, 14, 14)	50176	(1, 1024, 14, 14)	200704	262144	51380224



# 94	layer4.0.conv1	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 512, 14, 14)	100352	524288	102760448
# 95	layer4.0.conv2	Conv2d	k=(3, 3)	(1, 512, 14, 14)	100352	(1, 512, 7, 7)	25088	2359296	115605504
# 96	layer4.0.conv3	Conv2d	k=(1, 1)	(1, 512, 7, 7)	25088	(1, 2048, 7, 7)	100352	1048576	51380224
# 97	layer4.0.downsample.0	Conv2d	k=(1, 1)	(1, 1024, 14, 14)	200704	(1, 2048, 7, 7)	100352	2097152	102760448
# 98	layer4.1.conv1	Conv2d	k=(1, 1)	(1, 2048, 7, 7)	100352	(1, 512, 7, 7)	25088	1048576	51380224
# 99	layer4.1.conv2	Conv2d	k=(3, 3)	(1, 512, 7, 7)	25088	(1, 512, 7, 7)	25088	2359296	115605504
# 100	layer4.1.conv3	Conv2d	k=(1, 1)	(1, 512, 7, 7)	25088	(1, 2048, 7, 7)	100352	1048576	51380224
# 101	layer4.2.conv1	Conv2d	k=(1, 1)	(1, 2048, 7, 7)	100352	(1, 512, 7, 7)	25088	1048576	51380224
# 102	layer4.2.conv2	Conv2d	k=(3, 3)	(1, 512, 7, 7)	25088	(1, 512, 7, 7)	25088	2359296	115605504
# 103	layer4.2.conv3	Conv2d	k=(1, 1)	(1, 512, 7, 7)	25088	(1, 2048, 7, 7)	100352	1048576	51380224


# 104	fc	Linear		(1, 2048)	2048	(1, 1000)	1000	2048000	2048000
# Total sparsity: 0.00
#
# --- validate (epoch=249)-----------
# 10000 samples (256 per mini-batch)
# ==> Top1: 92.460    Top5: 99.670    Loss: 0.381
#
# ==> Best Top1: 92.580 on Epoch: 248
# Saving checkpoint to: logs/2018.11.29-145258/checkpoint.pth.tar
# --- test ---------------------
# 10000 samples (256 per mini-batch)
# ==> Top1: 92.460    Top5: 99.670    Loss: 0.381




version: 1
pruners:
  filter_pruner_70:
    class: 'L1RankedStructureParameterPruner'
    group_type: Channels
    desired_sparsity: 0.7
    group_dependency: Leader
    weights: [
      layer1.1.conv1.weight,
      layer1.0.conv1.weight,
      layer1.2.conv1.weight,
      layer1.3.conv1.weight,
      layer2.0.conv1.weight,
      layer2.0.downsample.0.weight
      ]

  filter_pruner_60:
    class: 'L1RankedStructureParameterPruner'
    group_type: Channels
    desired_sparsity: 0.6
    weights: [
      layer2.1.conv2.weight,
      layer2.2.conv2.weight,
      layer2.3.conv2.weight]

  filter_pruner_20:
    class: 'L1RankedStructureParameterPruner'
    group_type: Channels
    desired_sparsity: 0.2
    weights: [layer3.1.conv2.weight]

  filter_pruner_40:
    class: 'L1RankedStructureParameterPruner'
    group_type: Channels
    desired_sparsity: 0.4
    weights: [
      layer3.2.conv2.weight,
      layer3.3.conv2.weight,
      layer3.5.conv2.weight,
      layer3.6.conv2.weight,
      layer3.7.conv2.weight,
      layer3.8.conv2.weight,
      layer3.9.conv2.weight,
      layer3.10.conv2.weight,
      layer3.11.conv2.weight,
      layer3.12.conv2.weight,
      layer3.13.conv2.weight,
      layer3.14.conv2.weight,
      layer3.15.conv2.weight,
      layer3.16.conv2.weight,
      layer3.17.conv2.weight,
      layer3.18.conv2.weight,
      layer3.19.conv2.weight,
      layer3.20.conv2.weight,
      layer3.21.conv2.weight,
      layer3.22.conv2.weight
      ]


extensions:
  net_thinner:
      class: StructureRemover
      thinning_func_str: remove_channels
      arch: resnet101
      dataset: imagenet

lr_schedulers:
   exp_finetuning_lr:
     class: ExponentialLR
     gamma: 0.95


policies:
  - pruner:
      instance_name: filter_pruner_70
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_60
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_40
    epochs: [0]

  - pruner:
      instance_name: filter_pruner_20
    epochs: [0]

  - extension:
      instance_name: net_thinner
    epochs: [0]

  - lr_scheduler:
      instance_name: exp_finetuning_lr
    starting_epoch: 10
    ending_epoch: 300
    frequency: 1
